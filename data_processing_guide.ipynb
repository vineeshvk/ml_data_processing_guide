{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://cf-courses-data.static.labs.skills.network/jupyterlite/latest/lab/index.html?notebook_url=https%3A%2F%2Fcf-courses-data.static.labs.skills.network%2FIBM-ML0232EN-SkillsNetwork%2Flabs%2Fmodule%25202%2Flabs%2FData_Cleaning_Lab.jupyterlite.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- Initial Analysis\n",
    "- Finding Correlation\n",
    "- Making it normally distributed\n",
    "- Handling Duplicates\n",
    "- Handling Missing Data\n",
    "- Feature Scaling\n",
    "- Handling Outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More operations (Not added yet)\n",
    "- Categorical Encoding\n",
    "- Feature Engineering\n",
    "- Imbalance data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just analyze the data set first by using `info()` function.\n",
    "Then Use the `describe()` function to check for individual columns.\n",
    "\n",
    "\n",
    "The main observation we need to do is the min largely differs from the 25th percentile. Same for the max with 75th percentile If over or under it, means might not be normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df[\"Column\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have `describe()` used for numerical data. We can use `value_count()` for categorical data.\n",
    "It returns the counts of unique values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"categorical data\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step will be check the correlation of the features to the target value. We can use the seaborns heatmap to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out non correlated value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if we need to include only the numerical value\n",
    "df_num = df.select_dtypes(include = ['float64', 'int64']) \n",
    "\n",
    "# -1 ignoring the Target to target correlation\n",
    "df_num_corr = df_num.corr()['Target'][:-1] \n",
    "\n",
    "#displays pearsons correlation coefficient greater than 0.5\n",
    "top_features = df_num_corr[abs(df_num_corr) > 0.5].sort_values(ascending=False) \n",
    "\n",
    "print(\"There is {} strongly correlated values with Target:\\n{}\".format(len(top_features), top_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further manually filtering out the lower correlated value by visualizing a pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(df_num.columns), 5):\n",
    "    sns.pairplot(data=df_num,\n",
    "                x_vars=df_num.columns[i:i+5],\n",
    "                y_vars=['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a normal distribution curve of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the distribution has skewness measure it using the `skew()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: The range of skewness for a fairly symmetrical bell curve distribution is between -0.5 and 0.5; \n",
    "> moderate skewness is -0.5 to -1.0 and 0.5 to 1.0; \n",
    "> and highly skewed distribution is < -1.0 and > 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it's skewed we can make it normally distributed using `np.log()` or `np.sqrt()`\n",
    "\n",
    "More info: https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformed = np.log(df['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Duplicate Values using `duplicated([])`\n",
    "and drop duplicates using `drop_duplicates()`\n",
    "\n",
    "Alternative check for duplicates is `index.is_unique`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = df[df.duplicated(['Id'])]\n",
    "dup_removed = df.drop_duplicates()\n",
    "\n",
    "df.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Missing Data\n",
    "\n",
    "To find and visualize the missing values you can use `isnull()` and sum and finally plot it in a **bar plot** for vizualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df.isnull().sum().sort_values(ascending=False)\n",
    "total_select = total.head(20)\n",
    "\n",
    "total_select.plot(kind=\"bar\", figsize = (8,6), fontsize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to handle missing data\n",
    "- Remove the data\n",
    "- Impute the data: Substittute with mean, median or other estimation methods\n",
    "- Mask the data - Create a category for the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove data\n",
    "\n",
    "Either drop the columns where the value is null for a specfic column  \n",
    "or  \n",
    "Drop the whole column if it's not necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"Column\"])\n",
    "df.drop(\"Column\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impute data\n",
    "Fill the missing value with median or mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df[\"Column\"].median()\n",
    "df[\"Column\"].fillna(median, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "If all the features are at different scales it is better to equally scale them. Very important for anything regarding distance calculations such as K nearest neighbor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are mainly two types of scaling:\n",
    "- Normalization \n",
    "- Standardization\n",
    "\n",
    "\n",
    "Use the functionality from the sklearn lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinMaxScaler().fit_transform(df)\n",
    "StandardScaler().fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers\n",
    "\n",
    "\n",
    "Handling the outliers, this is a bit tricky. And has many options. \n",
    "Transformation seems to be the best way. (Log). Even that doesn't seem to be that effective.\n",
    "\n",
    "\n",
    "And sometimes it's real data and you don't want to change it. Because it might affect the desired outcome\n",
    "\n",
    "\n",
    "\n",
    "##### Handle Outliers\n",
    "\n",
    "- Remove the row\n",
    "- Substitute value\n",
    "- Transform (Log)\n",
    "- Predict(from other features, regression)\n",
    "- Keep value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unilateral analysis\n",
    "Use box plot to check if there is any outliers visually\n",
    "\n",
    "\n",
    "Also use describe to check the percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=housing['Column'])\n",
    "df['Column'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bilateral analysis\n",
    "Using a scatter plot to check the outliers and if it also follows the general trend.If it doens't follow the trend then remove it is the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x=\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Z score analysis\n",
    "\n",
    "This seems to be the best way to find the outliers\n",
    "\n",
    "> The usual value should be between -3 to 3 and if it's outside that we should remove it\n",
    "\n",
    "------\n",
    "**scipy** has the `zscore()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.zscore(df['Column']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
